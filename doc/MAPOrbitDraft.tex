\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{amssymb,amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{apacite}

\title{Finding the maximum-a-posteriori expressed behaviour of agents in an agent-based model\\
*** UNFINISHED DRAFT ***}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
  Daniel Tang\\
  Leeds Institute for Data Analytics\\
  University of Leeds\\
  Leeds, UK\\
  \texttt{D.Tang@leeds.ac.uk} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
}

\begin{document}
\maketitle

\begin{abstract}
We often make partial, noisy observations of some dynamical system and wish to explain those observations in terms of the most probable set of events that caused them. Here we consider the case when the dynamical system is a (possibly stochastic) time-stepping agent-based model with a discrete state space, the  (possibly noisy) observations are the number of agents that have some given property and the events are the expressed behaviours of the agents.

We show that this problem can be reduced to an integer linear programming problem which can subsequently be solved numerically using a standard branch-and-cut algorithm. We describe two implementations, an ``offline'' algorithm that finds the maximum-a-posteriori expressed behaviours given a set of observations over a finite time window, and an ``online'' algorithm that incrementally builds a feasible set of behaviours from a stream of observations that may have no natural beginning or end.

We demonstrate both algorithms on a spatial predator-prey model on a 32x32 grid with an initial population of 100 agents.

\textbf{This is an unfinished draft which may contain errors and is subject to change.}
\end{abstract}

% keywords can be removed
\keywords{Data assimilation, Agent based model, Quantum field theory, Probabilistic programming}

\section{Introduction}
%##########################################

Intuitively, the problem we'll consider here is as follows: there is a complex-system of interest which we believe can be well modelled by an agent based model. We have some prior beliefs about the boundary conditions of the system and have made some observations over some period of time. However, the observations are incomplete (in that they do not specify the full state of the model at any time) and possibly noisy. The problem is to find the maximum-a-posteriori (MAP) behaviour expressed by each agent given the observations. The expressed behaviours should consist of a feasible set of agent actions or \textit{events} that lead up to, or \textit{explain}, the observations.

In order to make an algorithm to solve this problem, we'll first define the problem a bit more precisely.

\subsection{What is an agent-based model?}
%#########################################

We take the essential characteristic of an agent-based model to be that the dynamics of the model can be described entirely in terms of agent behaviours (if the environment contains objects, these are also modelled as agents). Each agent may have an internal state and the state of the whole model at any instant can be completely specified by the states of all the agents.

Within this broad definition, there are a number of sub-categorizations
\begin{itemize}

\item Modelled time can be either discrete or continuous. In a discrete-time model, agent behaviour consists of timesteps from time $t$ to $t+1$ and time is an integer. In a continuous-time model agent behaviour consists of discrete events that can happen at any time expressed as a real number. In this paper we will consider discrete-time models.

\item The internal state of an agent can be discrete, continuous or mixed depending on whether the state can be most naturally expressed as a vector of integers, real numbers or a mixture of both. In this paper we will consider agents with discrete-state. 

\item Agents can be deterministic or stochastic. A deterministic agent will always exhibit the same behaviour if its own internal state and the states of other agents in the model are fixed, whereas a stochastic agent will exhibit different behaviours randomly drawn from a fixed distribution under the same circumstances. In this paper we will consider stochastic agents.

\item If only one agent can occupy a given state at any instant, then the state of the model can be described as a set of agent states and we call it a Fermi-Dirac model, whereas if two or more agents can occupy the same state at the same time then the model state must be described as a multiset\cite{blizard1988multiset} of agent states and we call it a Bose-Einstein model. In this paper we will consider Bose-Einstein models.

\end{itemize}

We define the behaviour of an agent as a computer program equipped with the ability to
\begin{itemize}
\item query and modify its own state
\item query whether there exist any agents in the model that satisfy any given predicate on their state
\item create a new agent in any state
\item remove itself from the model
\item call a function that returns a random number.
\end{itemize}
Such a program can be expressed as a conditional probability distribution over a set of events \cite{staton2016semantics}. Each event represents an agent performing an action that expresses a specific behaviour, and has the form $(\Phi|\psi,R,\bar{R})$ where $\psi$ is the internal state of the agent before the behaviour is expressed, $R$ is the set of other agents (states) that must be present for the behaviour to be expressed, $\bar{R}$ is the set of agents that must be absent for the behaviour to be expressed and $\Phi$ is the multiset of agents that are the result of the action occurring (including the final state of the acting agent). So, if we say an agent has a behaviour $P(\Phi|\psi,R,\bar{R})$ then when it is in state $\psi$ in an environment containing all agents in $R$ and no agents in $\bar{R}$, it performs an action that results in $\Phi$ with probability $P(\Phi|\psi,R,\bar{R})$.

So, for example, an agent with behaviour $P(\{\psi'\}|\psi,\emptyset,\emptyset) = 1.0$ will, with certainty, change its state to $\psi'$ at time $t+1$ if it is in state $\psi$ at time $t$. An agent with behaviour $P(\emptyset|\psi,\{\phi\},\emptyset) = 0.5$ in state $\psi$ in the presence of another agent in state $\phi$ will disappear with probability $0.5$.

Without loss of generality, we assume that all agents in the model have the same behaviour. If necessary, multiple types of agent can be modelled by making the agent's type part of its state.

In any timestep of the model a multiset of events will occur, we will call such a multiset a \textit{model event} to distinguish it from the \textit{agent events} that are members of the model event.

We define an event trajectory as a list of model events $\left<E_1...E_n\right>$, which can be thought of as the assertion that, for each $i$, the events in $E_i$ occurred in timestep from $t=i-1$ to $t=i$. 

If a model is in state $\Psi$, only certain model events are possible so we say that $\Psi$ \textit{satisfies} a model event $E$, and write $\Psi\vdash E$, if the following conditions hold:

Each agent in $\Psi$ and must perform exactly one action per timestep
\begin{equation}
\sum_{(\Phi|\psi,R,\bar{R})\in E}\{\psi\} = \Psi
\label{agency}
\end{equation}
...each event's presence-requirements must be fulfilled
\[
\bigcup_{(\Phi|\psi,R,\bar{R})\in E} R \subseteq \Psi
\]
...and each event's absence-requirements must be fulfilled
\[
\left(\bigcup_{(\Phi|\psi,R,\bar{R})\in E_t} \bar{R}\right) \cap \Psi = \emptyset
\]
The consequence of a model event is the sum of the consequences of its members. We'll write this as $\Psi(E)$
\begin{equation}
\Psi(E) = \sum_{(\Phi|\psi,R,\bar{R})\in E}\Phi
\label{psie}
\end{equation}

The consequences of an event trajectory, $\left<E_1...E_n\right>$, is a trajectory of states $\left<\Psi(E_1)...\Psi(E_2)\right>$ and the trajectory is feasible if
\[
\forall i\in 2...n: \Psi(E_{i-1}) \vdash E_i
\]

The probability of a feasible model event is given by
\[
P(E|\Psi) = \prod_{(\Phi|\psi,R,\bar{R}) \in E} P(\Phi|\psi,R,\bar{R})
\]
So, if the model is in state $\Psi_t$ at time $t$ then the set of possible timesteps is the set of model events that are satisfied by $\Psi$, which will each occur with probability $P(E|\Psi)$. At time $t+1$, after the occurence of model event $E$, the state will be $\Psi_{t+1} = \Psi(E)$.


\subsection{What is an observation?}
%###################################

In this paper we will consider observations that consist of a count of the number of agents for which some predicate is true. The observation can be exact or noisy in that the real count can lie in some range. More formally $\omega = \left<L,U,B\right>$ denotes the observation that the number of agents for which predicate $B$ is true is somewhere in the range  $L \le N \le U$. If $\Psi$ is a model state,  then we say that $\Psi$ satisfies the observation, which we write as $\Psi \vdash \omega_i$, if and only if
\[
L \le \left|\left\{\psi:\psi \in \Psi \wedge B(\psi)\right\}\right| \le U
\]
From this, we say that $\Psi$ satisfies a set of observations, $\Omega$, if it satisfies all of its members, and a trajectory of states $\mathcal{T} = \left<\Psi_1...\Psi_n\right>$ satisfies a list of sets of observations (which we'll call an observation of a trajectory), $\mathcal{O} = \left<\Omega_1...\Omega_n\right>$ if for each $i$, $\Psi_i$ satisfies $\Omega_i$.

This form of observation can deal with exact observations ($L=U$), observations where there may be false-positives ($U$ is the observed count and $L=0$), observations where there may be false negatives ($L$ is the observed count and $U=\infty$) or observations that have some uncertainty ($L = N - \Delta N$ and $U = N + \Delta N$).
 
\subsection{What is a prior belief about boundary conditions?}
%###################################

The boundary conditions on the system consist of agents entering the system at particular times. At its simplest, it is a set of agents that enter the system at time $t=0$, though they need not all enter at this time.

More generally, it is a set of probability distributions $P(\Gamma,t)$ which give the prior probability that a multiset $\Gamma$ is a subset of the agents that enter the system at time $t$.

In this paper we assume that these prior beliefs can be factorised into factors that involve single agent states. In this case the prior probabilities can be expressed as
\[
P(\Gamma,t) = \prod_{\psi \in^{=n} \Gamma} P(\psi,n,t)
\]
where $\psi\in^{=n}\Gamma$ is true when $\psi$ appears in $\Gamma$ exactly $n$ times and $P(\psi,n,t)$ is the probability of at least $n$ agents in state $\psi$ being injected at time $t$. 

Boundary conditions can be conveniently represented in terms of \textit{injection events} which are caused by a set of hypothetical (though never explicitly represented) \textit{zero energy} agents $\emptyset_\psi$ which, by definition, are present before $t=0$. An injection event , $(\{\emptyset_\psi, \psi^n\}|\emptyset_\psi,\emptyset,\emptyset)$, has no requirements and results in $n$ agents in the state $\psi$ being injected into the system, along with the continued existence of the injecting agent. If we allow the behaviour of these agents to be time dependent, then any set of injection events can be represented by supposing the existence of zero energy agent at time $t=-1$ for each state that needs injection.

With zero energy agents we can now conveniently define an event trajectory along with its boundary conditions as a list of model events $\mathcal{E} = \left<E_0...E_n\right>$ which induces a model state trajectory $\mathcal{T} = \left<\Psi(E_0)...\Psi(E_n)\right>$ and which is feasible if and only if $\{\emptyset_{\psi_1}...\emptyset_{\psi_n}\} \vdash E_0$ and  $\forall i \in [1,n]: \Psi(E_{i-1}) \vdash E_i$

\subsection{What is an explanation of a set of observations?}
%############################################################

Armed with the above definitions, we can define an ``explanation'' of an observation of a trajectory, $\mathcal{O} = \left<\Omega_0...\Omega_n\right>$, as a feasible event trajectory $\mathcal{E} = \left<E_0...E_n\right>$ whose associated state trajectory $\mathcal{T} = \left<\Psi(E_0)...\Psi(E_n)\right>$ satisfies the observation $\mathcal{O}$.

The probability of an explanation is just the product of the probabilities of all the events
\[
P(\left<E_0...E_m\right>) = \prod_{i}\prod_{\epsilon\in E_i, }P(\epsilon)
\]

\section{Finding the MAP event trajectory}
%#####################################################

Bringing all this together, we can now express our problem precisely:

Given a set of observations $\Omega$ and an agent behaviour $P(\Phi|\psi,R,\bar{R})$, find
\[
\mathcal{E} = \arg\max_{\left<E_0...E_m\right>}\prod_{i}\prod_{\epsilon\in E_i, }P(\epsilon)
\]
subject to $\mathcal{E}$ being feasible and it's implied state trajectory satisfying $\Omega$.

To turn this into a tractable algorithm, first represent an event trajectory as a set of non-negative integers $c_{te}$ such that in timestep $t$, the event $e$ occurs exactly $c_{te}$ times.

Now encode each event in an agent's behaviour, $e = (\Phi|\psi,R,\bar{R})$, by letting 
\[
\Phi_{e\phi} = n: \phi \in^n \Phi\\
\]
\[
\psi_{e\phi} = 
\begin{cases}
1&\text{if } \phi = \psi\\
0&\text{otherwise}\\
\end{cases}
\]
\[
R_{e\phi} =
\begin{cases}
1&\text{if } \phi \in R\\
0&\text{otherwise}\\
\end{cases}
\]
\[
\bar{R}_{e\phi} =
\begin{cases}
1&\text{if } \phi \in \bar{R}\\
0&\text{otherwise}\\
\end{cases}
\]
Now encode the observation predicates for each event's consequences
\[
B_e = |\{\psi: \psi\in\Phi \wedge B(\psi)\}|
\]
and encode the probability of each event
\[
P_e = P(\Phi|\psi,R,\bar{R})
\]
finally, define a set of boolean variables $0 \le b_{t\phi} \le 1$ which satisfy
\[
Mb_{t\phi} \ge \sum_e  \Phi_{e\phi}c_{(t-1)e}
\]
\[
b_{t\phi} \le \sum_e  \Phi_{e\phi}c_{(t-1)e}
\]
for some \textit{multiplicity} $M$ which can be any number larger than the expected maximum number of agents in the model at any one time.

We can now express the problem as:

Maximise
\[
P = \sum_t\sum_e \log(P_e)c_{te}
\]
subject to the observations being satisfied
\[
\forall \Omega_t \in \mathcal{O}, \left<L,U,B\right> \in \Omega_t: L \le \sum_e B_e c_{te} \le U
\]
...each agent performing exactly one action
\begin{equation}
\forall\phi: 0 \le \sum_e  \Phi_{e\phi}c_{(t-1)e} -  \sum_e\ \psi_{e\phi} c_{te} \le 0
\label{IPagency}
\end{equation}
...each event's presence requirements being fulfilled
\[
\forall\phi: 0 \le Mb_{t\phi} - \sum_e\ R_{e\phi} c_{te} 
\]
...each event's absence requirements being fulfilled
\[
\forall\phi: \sum_e\ \bar{R}_{e\phi} c_{te} + Mb_{t\phi} \le M
\]
This can be seen to be in the form of a pure integer linear programming problem, which in full generality is NP complete, but for which there exists many algorithms that, in practice, are able to find solutions to reasonably large problems.


\section{Online assimilation}
%############################

When using a model to make forecasts of the real world, observations arrive in a constant stream which may have no specific start or end. In this case, we would usually split the stream up into time \textit{windows} and update the forecasts one window at a time. However, if we commit to the maximum-a-posteriori explanation of one window, and use the end state of this explanation as the start state of the next window, we are in danger of being led down ``garden paths''; i.e. the observations lead us into believing the most probable explanation, only to later show us that a much less probable and completely incompatible explanation is in-fact the case. If the garden path is contained within a single window then the optimisation will deal with it, but if it stretches across windows, we may commit to believing events which, in the light of more evidence, turn out to be impossible. At this point the assimilation must stop because there is no event trajectory that satisfies the observations for the current window. To solve this problem, we present a second algorithm for use with streaming observations.

We begin by defining the idea of a \textit{partial event trajectory} which is like a normal event trajectory except that instead of each agent having to perform exactly one action per timestep, they can perform at most one action per timestep. So, in the definition of satisfaction, in place of equation \ref{agency} we have
\begin{equation}
\sum_{(\Phi|\psi,R,\bar{R})\in E}\{\psi\} \subseteq \Psi
\end{equation}
In this case, we'll say that $\Psi$ partially satisfies $E$. Intuitively, a partial event trajectory can be interpreted as saying that at least these events happened in these timesteps, but other additional events may have happened concomitantly.

This translates to the Integer programming constraint
\begin{equation}
\forall\phi: 0 \le \sum_e  \Phi_{e\phi}c_{(t-1)e} -  \sum_e\ \psi_{e\phi} c_{te}
\end{equation}
in place of equation \ref{IPagency}.

A solution to the resulting problem is a partial event trajectory that makes as few commitments as possible while satisfying the observations. In a partial trajectory, only a subset of the agents at any particular time will be moved forward by the next model event, so agents may be left hanging at a time in the middle of the trajectory. This is good, because it reduces the likelihood that we will be led down a garden path by delaying any commitment to the exhibited behaviour of the hanging agents until more observations are available.

Once we've found a partial trajectory, we take the next window of observations and find the maximum-a-posteriori partial trajectory given these new observations. In order to correctly move hanging agents forward in time, they can be treated as part of the injected boundary conditions. However, because we have now committed to events in the hanging agents' future, we need to add the additional constraint that the absences we have committed to must be respected. This is easily done by removing the indicator variable $b_{t\phi}$ and adding the constraint
\[
0 \le \sum_e  \Phi_{e\phi}c_{(t-1)e} \le 0
\]
for each state $\phi$ that must be absent at time $t$.

Even though partial trajectories reduce the probability of being led down the garden path, they do not eradicate it completely. So, if we end up with observations that seem impossible, we rollback the commitments, one timestep at a time, from the most recent until a solution becomes possible. In the worst case, we'll have to roll-back to $t=0$, but in practice the probability of this will be very small for most models.

One final issue with online assimilation is dying agents (or agents that leave the model for some other reason). When an agent dies it will not be observed again, but the online assimilation will just leave it in the hanging state on the off-chance that it's still around and may be observed in the future. At some point we are likely to want to commit to saying that the agent has died. To do this we can estimate the probability of the agent evading observation for so long, and when this estimate falls below a certain level, we commit to it's death.

\section{Experiments with a spatial predator-prey model}
%########################################################

In order to demonstrate the above algorithms, we simulated a spatial predator-prey model on a 32x32 grid with periodic boundary conditions. Agents can be either predator or prey. At each timestep, agents can stay on the same grid-square, move to an adjacent grid-square (i.e. up, down, left or right) or die. Prey can also give birth to another prey on an adjacent grid-square. Prey cannot move onto a square that contains a predator, but in the presence of a predator on an adjacent square they may be eaten and the predator reproduce into the newly vacant square. The probability of each behaviour is shown in table 1. Perhaps surprisingly, even a model with such a simple set of behaviours exhibits quite complex emergent properties that are difficult to predict.


\begin{table}
\begin{center}
\begin{tabular}{llc}
\hline
Aget type & Description & Probability\\
\hline
Prey & die &        0.03\\
 & reproduce &        0.06\\
 & move (up/down/left/right) / be eaten &        0.728\\
 & stay put / be eaten &  0.182 \\
 &&\\
Predator & die  &      0.05\\
 & move (up/down/left/right)&        0.76\\
 & stay put & 0.19\\
\hline
\end{tabular}
\end{center}
\caption{The rates of each behaviour in the predator-prey model}
\label{rates}
\end{table}



A forward simulation of the ABM was performed in order to create observation data and boundary conditions. The boundary conditions consisted of a fully observed initial state while in subsequent timesteps each agent was observed with a probability 0.5.

\subsection{Offline MAP experiment}

Using the boundary conditions and observations for five timesteps, we set up an integer linear programming problem as specified above and used the Google's OR-Tools\cite{googleortools} to solve the problem and find the maximum-a-posteriori trajectory. A capture of a typical configuration of the real state and the MAP at the end of the five timesteps is shown in figure ... Figure ... shows the average over xxx simulations of the Manhattan distance between the unobserved agents in the MAP and the nearest agent of the same type in the real state. For comparison, we also show the average distance for a model that places the agents randomly with uniform probability.

\subsection{Online assimilation experiment}




\section{Conclusion}
%###################

We have demonstrated that the maximum-a-posteriori trajectory of an agent based model can be found for a window of partial, noisy observaations of a spatial predator-prey model. We have also shown how to construct a feasible interpretation of a stream of such observations of arbitrary length.

Since integer linear programming problems are, in full generality, NP-complete, the technique described here will not scale to very large models but will work best when agents choose from one of a limited number of options at each timestep, when interactions are sparse and when the observations impose strong constraints on the state.

Finding the MAP trajectory can be useful for a number of reasons. If we're forced to commit to a specific trajectory, given some observations, the MAP is our best bet. However, it is the most probable trajectory (perhaps among others with the same probability) among a very large number of other less probable trajectories. So usually it is very unlikely that the MAP is what really happened and we certainly shouldn't, in general, use the MAP as an interpretation of what actually happened. However, the MAP trajectory can be useful in other ways; for example, it is often a good initial sample to start an MCMC sampling algorithm, or showing the non-existence of a MAP can show that there must be either a false observation or some process has occurred that 


%\bibliographystyle{unsrtnat}
%\bibliographystyle{apalike} 
\bibliographystyle{apacite}
\bibliography{references}

\end{document}
