\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{amssymb,amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{apacite}

\title{Finding the maximum-a-posteriori expressed behaviour of agents in an agent-based model\\
*** UNFINISHED DRAFT ***}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
  Daniel Tang\\
  Leeds Institute for Data Analytics\\
  University of Leeds\\
  Leeds, UK\\
  \texttt{D.Tang@leeds.ac.uk} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
}

\begin{document}
\maketitle

\begin{abstract}
In this paper we consider the problem of finding the most probable set of events that could have led to a set of partial, noisy observations of some dynamical system. In particular, we consider the case when the dynamical system is a (possibly stochastic) time-stepping agent-based model with a discrete state space, the  (possibly noisy) observations are the number of agents that have some given property and the events we're interested in are the expressed behaviours of the agents.

We show that this problem can be reduced to an integer linear programming problem which can subsequently be solved numerically using a standard branch-and-cut algorithm. We describe two implementations, an ``offline'' algorithm that finds the maximum-a-posteriori expressed behaviours given a set of observations over a finite time window, and an ``online'' algorithm that incrementally builds a feasible set of behaviours from a stream of observations that may have no natural beginning or end.

We demonstrate both algorithms on a spatial predator-prey model on a 32x32 grid with an initial population of 100 agents.

\textbf{This is an unfinished draft which may contain errors and is subject to change.}
\end{abstract}

% keywords can be removed
\keywords{Data assimilation, Agent based model, Quantum field theory, Probabilistic programming}

\section{Introduction}
%##########################################

Intuitively, the problem we'll consider here is as follows: there is a complex-system of interest which we believe can be well modelled by an agent based model. We have some prior beliefs about the boundary conditions of the system and have made some observations over some period of time. However, the observations are incomplete (in that they do not specify the full state of the model at any time) and possibly noisy. The problem is to find the maximum-a-posteriori (MAP) behaviour expressed by each agent given the observations. The expressed behaviours should consist of a feasible set of agent actions or \textit{events} that lead up to, or \textit{explain}, the observations.

Before presenting an algorithm to solve this problem, we'll first define the problem a bit more precisely.

\subsection{What is an agent-based model?}
%#########################################

We take the essential property of an agent-based model to be that the dynamics of the model can be described entirely in terms of agent behaviours (if the environment contains objects, these are also modelled as agents). Each agent may have an internal state and the state of the whole model at any instant can be completely specified by the states of all the agents.

Within this broad definition, there are a number of sub-categorizations
\begin{itemize}

\item Modelled time can be either discrete or continuous. In a discrete-time model, agent behaviour consists of timesteps from time $t$ to $t+1$ and time is an integer. In a continuous-time model agent behaviour consists of discrete events that can happen at any time expressed as a real number. In this paper we will consider discrete-time models. We will also suppose that every agent gets an opportunity to act in each timestep, and that each agent potentially has access to the states of other agents at time $t$ when making the timestep to $t+1$.

\item The internal state of an agent can be discrete, continuous or mixed depending on whether the state can be most naturally expressed as a vector of integers, real numbers or a mixture of both. In this paper we will consider agents with discrete-state. 

\item Agents can be deterministic or stochastic. A deterministic agent will always exhibit the same behaviour if its own internal state and the states of other agents in the model are fixed, whereas a stochastic agent will exhibit different behaviours randomly drawn from a fixed distribution under the same circumstances. In this paper we will consider stochastic agents.

\item If only one agent can occupy a given state at any instant, then the state of the model can be described as a set of agent states and we call it a Fermi-Dirac model, whereas if two or more agents can occupy the same state at the same time then the model state must be described as a multiset\cite{blizard1988multiset} of agent states and we call it a Bose-Einstein model. In this paper we will consider Bose-Einstein models.

\end{itemize}

We define the behaviour of an agent as a computer program which describes a timestep of the agent. The program may
\begin{itemize}
\item query its own state at time $t$
\item query whether there exist any agents in the model at time $t$ that satisfy any given predicate on their state
\item create a new agent in any state
\item remove itself from the model
\item modify its own state
\item call a function that returns a random number.
\end{itemize}
A timestep of the whole model consists of the execution of the timestepping program for each agent.

Such a program defines a conditional probability distribution over a set of events \cite{staton2016semantics}. Each event represents an agent performing an action that expresses a specific behaviour, which we will write as $(\Phi|\psi,R,\bar{R})$ where $\psi$ is the internal state of the agent before the behaviour is expressed, $R$ is the set of other agents (states) that must be present for the behaviour to be expressed, $\bar{R}$ is the set of agents that must be absent for the behaviour to be expressed and $\Phi$ is the multiset of agents that are the result of the action occurring (including the final state of the acting agent). So, if we say an agent has a behaviour $P(\Phi|\psi,R,\bar{R})$ then when it is in state $\psi$ in an environment containing all agents in $R$ and no agents in $\bar{R}$, it performs an action that results in $\Phi$ with probability $P(\Phi|\psi,R,\bar{R})$.

So, for example, an agent in state $\psi$ at time $t$ with behaviour $P(\{\psi'\}|\psi,\emptyset,\emptyset) = 1.0$ will, with certainty, change its state to $\psi'$ at time $t+1$ . An agent with behaviour $P(\emptyset|\psi,\{\phi\},\emptyset) = 0.5$ in state $\psi$ in the presence of another agent in state $\phi$ will disappear with probability $0.5$.

Without loss of generality, we assume that all agents in the model have the same behaviour. If necessary, multiple types of agent can be modelled by making the agent's type part of its state.

In any timestep of the model a multiset of events will occur, we will call such a multiset a \textit{model event} to distinguish it from the \textit{agent events} that are members of the model event. The consequence of a model event, $E$, is the sum of the consequences of its members. We'll write this as $\Psi(E)$
\begin{equation}
\Psi(E) = \sum_{(\Phi|\psi,R,\bar{R})\in E}\Phi
\label{psie}
\end{equation}

We define an event trajectory as a list of model events $\left<E_1...E_n\right>$, which can be thought of as the assertion that, for each $i$, the events in $E_i$ occurred in timestep from $t=i-1$ to $t=i$. Not all event trajectories are feasible as, for each agent event, the presence and absence conditions of the event must be met.

Let an event trajectory, $\left<E_1...E_n\right>$ is feasible if and only if $\forall i\in 2...n:$

Each agent in $\Psi$ must perform exactly one action per timestep\footnote{where $\{\psi\}$ is a multiset and the sum of multisets is defined in the obvious way}
\begin{equation}
\sum_{(\Phi|\psi,R,\bar{R})\in E_i}\{\psi\} = \Psi(E_{i-1})
\label{agency}
\end{equation}
...each event's presence-requirements must be fulfilled
\[
\bigcup_{(\Phi|\psi,R,\bar{R})\in E_i} R \subseteq \Psi(E_{i-1})
\]
...and each event's absence-requirements must be fulfilled
\[
\left(\bigcup_{(\Phi|\psi,R,\bar{R})\in E_i} \bar{R}\right) \cap \Psi(E_{i-1}) = \emptyset
\]


The probability of a feasible model event is given by
\[
P(E) = \prod_{(\Phi|\psi,R,\bar{R}) \in E} P(\Phi|\psi,R,\bar{R})
\]
and the probability of an event trajectory is just the product of the probabilities of all the events
\[
P(\left<E_0...E_m\right>) = \prod_{i}\prod_{\epsilon\in E_i, }P(\epsilon)
\]


\subsection{What is an observation?}
%###################################

In this paper we will consider observations that consist of a count of the number of agents for which some predicate is true. The observation can be exact or noisy in that the real count can lie in some range. More formally $\omega = \left<L,U,B\right>$ denotes the observation that the number of agents, $N$, for which predicate $B$ is true is somewhere in the range  $L \le N \le U$. If $\Psi$ is a model state,  then we say that $\Psi$ satisfies the observation, which we write as $\Psi \vdash \omega_i$, if and only if
\[
L \le \left|\left\{\psi:\psi \in \Psi \wedge B(\psi)\right\}\right| \le U
\]
From this, we say that $\Psi$ satisfies a set of observations, $\Omega$, if it satisfies all of its members. We can extend this to model event trajectories $\mathcal{E} = \left<E_1...E_n\right>$ and say that $\mathcal{E}$ satisfies a list of sets of observations (which we'll call an observation of a trajectory), $\mathcal{O} = \left<\Omega_1...\Omega_n\right>$ if for each $i$, $\Psi(E_i)$ satisfies $\Omega_i$.

This form of observation can deal with exact observations ($L=U$), observations where there may be false-positives ($U$ is the observed count and $L=0$), observations where there may be false negatives ($L$ is the observed count and $U=\infty$) or observations that have some uncertainty range ($L = N - \Delta N$ and $U = N + \Delta N$).
 
\subsection{What is a prior belief about boundary conditions?}
%###################################

The boundary conditions on the system consist of agents entering the system at particular times. At its simplest, it is a set of agents that enter the system at time $t=0$, though they need not all enter at this time.

More generally, it is a set of probabilities $P(\Gamma,t)$ which give the prior probability that a multiset $\Gamma$ is a subset of the agents that enter the system at time $t$.

In this paper we assume that these prior beliefs can be factorised into factors that involve single agent states. In this case the prior probabilities can be expressed as
\[
P(\Gamma,t) = \prod_{\psi \in^{=n} \Gamma} P(\psi,n,t)
\]
where $\psi\in^{=n}\Gamma$ is true when $\psi$ appears in $\Gamma$ exactly $n$ times and $P(\psi,n,t)$ is the probability of at least $n$ agents in state $\psi$ being injected at time $t$. 

Boundary conditions can be conveniently represented in terms of the behaviour of a set of hypothetical (though never explicitly represented) \textit{zero energy} agents $\emptyset_\psi$ which, by definition, are present before $t=0$. The behaviour of a zero energy agent has the form, $(\{\emptyset_\psi, \psi^n\}|\emptyset_\psi,\emptyset,\emptyset)$ which results in $n$ agents in the state $\psi$ being injected into the system, along with the continued existence of the injecting, zero energy agent. If we allow the behaviour of these agents to be time dependent, then any set of boundary conditions can be represented by supposing that there is a model event at time $t=0$ that consists entirely of zero energy agent events.



\section{An algorithm to find the MAP event trajectory}
%#####################################################

Bringing all this together, we can now express our problem precisely:

Given an observation of a trajectory $\mathcal{O}$ and an agent behaviour $P(\Phi|\psi,R,\bar{R})$, find
\begin{equation}
\mathcal{E} = \arg\max_{\left<E_0...E_m\right>}\prod_{i}\prod_{\epsilon\in E_i, }P(\epsilon)
\end{equation}
subject to $\mathcal{E}$ being feasible and satisfying $\mathcal{O}$.

To turn this into a tractable algorithm, first represent an event trajectory as a set of non-negative integers $c_{te}$ such that in timestep $t$, the event $e$ occurs exactly $c_{te}$ times.

Now encode each event in an agent's behaviour, $e = (\Phi|\psi,R,\bar{R})$, by letting 
\begin{equation}
\Phi_{e\phi} = n: \phi \in^n \Phi\\
\end{equation}

\begin{equation}
\psi_{e\phi} = 
\begin{cases}
1&\text{if } \phi = \psi\\
0&\text{otherwise}\\
\end{cases}
\end{equation}

\begin{equation}
R_{e\phi} =
\begin{cases}
1&\text{if } \phi \in R\\
0&\text{otherwise}\\
\end{cases}
\end{equation}

\begin{equation}
\bar{R}_{e\phi} =
\begin{cases}
1&\text{if } \phi \in \bar{R}\\
0&\text{otherwise}\\
\end{cases}
\end{equation}

Now encode the observation predicates for each event's consequences
\begin{equation}
B_e = |\{\psi: \psi\in\Phi \wedge B(\psi)\}|
\end{equation}
and encode the probability of each event
\[
P_e = P(\Phi|\psi,R,\bar{R})
\]
finally, let
\begin{equation}
\Psi_{t\phi} = \sum_e\Phi_{e\phi}c_{te}
\label{stateIndicator}
\end{equation}

and define a set of boolean variables $0 \le b_{t\phi} \le 1$ which satisfy
\begin{equation}
0 \le Mb_{t\phi} - \Psi_{t\phi}
\label{bGEconstraint}
\end{equation}
\begin{equation}
0 \le \Psi_{t\phi} - b_{t\phi} 
\label{bLEconstraint}
\end{equation}
for some \textit{multiplicity} $M$ which can be any number larger than the expected maximum number of agents in the model at any one time.

We can now express the problem as:

Maximise
\begin{equation}
P = \sum_t\sum_e \log(P_e)c_{te}
\end{equation}
subject to the observations being satisfied
\begin{equation}
\forall \Omega_t \in \mathcal{O}, \left<L,U,B\right> \in \Omega_t: L \le \sum_e B_e c_{te} \le U
\label{observation}
\end{equation}
...each agent performing exactly one action
\begin{equation}
\forall\phi: 0 \le \Psi_{(t-1)\phi} -  \sum_e\ \psi_{e\phi} c_{te} \le 0
\label{IPagency}
\end{equation}
...each event's presence requirements being fulfilled
\begin{equation}
\forall\phi: 0 \le Mb_{(t-1)\phi} - \sum_e\ R_{e\phi} c_{te} 
\end{equation}
...each event's absence requirements being fulfilled
\begin{equation}
\forall\phi: \sum_e\ \bar{R}_{e\phi} c_{te} + Mb_{(t-1)\phi} \le M
\label{absenceConstraint}
\end{equation}
This can be seen to be in the form of a pure integer linear programming problem, which in full generality is NP complete, but for which there exists many algorithms that, in practice, are able to find solutions to reasonably large problems.


\section{Online assimilation}
%############################

The algorithm described above assumes that the observations span a finite time window. If the observations span a very long window or originate from a stream which has no specific start or end, we would usually split the stream up into smaller windows and deal with the observations one window at a time. However, if we commit to the maximum-a-posteriori event trajectory of one window, and use the end state of this as the start state of the next window, we are in danger of being led down ``garden paths''; i.e. the observations lead us into believing the most probable explanation, only to later show us that a much less probable and completely incompatible explanation is in-fact the case. So, we can't commit to believing an event trajectory for one window because, in the light of the observations from the next window, the trajectory may turn out to be impossible. If this happens the assimilation must stop because there is no event trajectory that satisfies the new window's observations while being a feasible extension of the previous window's trajectory. To solve this problem, we present a second algorithm for use with streaming observations.

We begin by defining the idea of a \textit{partial event trajectory} which is like a normal event trajectory except that instead of each agent having to perform exactly one action per timestep, they can perform at most one action per timestep. So, in the definition of satisfaction, in place of equation \ref{agency} we have
\begin{equation}
\sum_{(\Phi|\psi,R,\bar{R})\in E_i}\{\psi\} \subseteq \Psi(E_{i-1})
\end{equation}
Intuitively, a partial event trajectory can be interpreted as saying that at least these events happened in these timesteps, but other additional events may have happened concomitantly.

In an partial trajectory, all observations are satisfied but only a subset of the agents present at any particular time will be moved forward by the next model event, so agents may be left hanging at a time in the middle of the trajectory. This is good, because it reduces the likelihood that we will be led down a garden path by delaying any commitment to the exhibited behaviour of the hanging agents until more observations are available.

The assimilation proceeds by finding a partial trajectory for one window, then taking the next window of observations and finding the maximum-a-posteriori partial trajectory given these new observations along with the partial trajectory we've already committed to. In order to correctly move hanging agents forward in time, we need to distinguish between the events that we've already committed to and the events we're optimising over. If we let $\hat{c}_{te}$ be the number of events of type $e$ that we've committed to in timestep $t$, then equation \ref{stateIndicator} becomes
\begin{equation}
\Psi_{t\phi} -  \sum_e\Phi_{e\phi}c_{te} = \sum_e\Phi_{e\phi}\hat{c}_{te}
\label{stateIndicator}
\end{equation}


the agency constraint, equation \ref{IPagency}, becomes
\begin{equation}
\forall\phi:  \sum_e\ \psi_{e\phi}\hat{c}_{te} \le \Psi_{(t-1)\phi}  -  \sum_e\ \psi_{e\phi} c_{te}
\end{equation}
and the absence constraint, equation \ref{absenceConstraint}, becomes
\begin{equation}
\forall\phi: \sum_e\ \bar{R}_{e\phi} c_{te} + Mb_{(t-1)\phi} \le M - \sum_e\ \bar{R}_{e\phi} \hat{c}_{te}
\end{equation}

Since observations are fully satisfied by a partial trajectory, the observation constraints, equation \ref{observation}, now only apply to timesteps for which there are no committed events. The presence requirement constraints can remain unchanged.

Even though partial trajectories reduce the probability of us being led down the garden path, they do not eradicate it completely. So, if we end up with observations that are incompatible with our commitments, we rollback the commitments, one timestep at a time, from the most recent until a solution becomes possible. In the worst case, we'll have to roll-back all the way to $t=0$, but in practice the probability of this will be very small for most models.

One final issue with online assimilation is dying agents (or agents that leave the model for some other reason). When an agent dies it will not be observed again, but the online assimilation will just leave it in the hanging state on the off-chance that it's still around but has evaded observation. At some point we are likely to want to commit to saying that the agent has died. To do this we can estimate the probability of the agent evading observation for so long, and when this estimate falls below a certain level, we commit to it's death.

\section{Experiments with a spatial predator-prey model}
%########################################################

In order to demonstrate the above algorithms, we simulated a spatial predator-prey model on a 32x32 grid with periodic boundary conditions. Agents can be either predator or prey. At each timestep, agents can stay on the same grid-square, move to an adjacent grid-square (i.e. up, down, left or right) or die. Prey can also give birth to another prey on an adjacent grid-square. Prey cannot move onto a square that contains a predator, but in the presence of a predator on an adjacent square they may be eaten and the predator reproduce into the newly vacant square. The probability of each behaviour is shown in table 1. Perhaps surprisingly, even a model with such a simple set of behaviours exhibits quite complex emergent properties that are difficult to predict.


\begin{table}
\begin{center}
\begin{tabular}{llc}
\hline
Aget type & Description & Probability\\
\hline
Prey & die &        0.03\\
 & reproduce &        0.06\\
 & move (up/down/left/right) / be eaten &        0.728\\
 & stay put / be eaten &  0.182 \\
 &&\\
Predator & die  &      0.05\\
 & move (up/down/left/right)&        0.76\\
 & stay put & 0.19\\
\hline
\end{tabular}
\end{center}
\caption{The rates of each behaviour in the predator-prey model}
\label{rates}
\end{table}



A forward simulation of the ABM was performed in order to create observation data and boundary conditions. The boundary conditions consisted of a fully observed initial state while in subsequent timesteps each agent was observed with a probability 0.5.

\subsection{Offline MAP experiment}

Using the boundary conditions and observations for five timesteps, we set up an integer linear programming problem as specified above and used the Google's OR-Tools\cite{googleortools} to solve the problem and find the maximum-a-posteriori trajectory. A capture of a typical configuration of the real state and the MAP at the end of the five timesteps is shown in figure ... Figure ... shows the average over xxx simulations of the Manhattan distance between the unobserved agents in the MAP and the nearest agent of the same type in the real state. For comparison, we also show the average distance for a model that places the agents randomly with uniform probability.

\subsection{Online assimilation experiment}

The online algorithm was applied to the same observations and boundary conditions as the offline experiment. The window size for each increment was one timestep. Figure xxx shows the difference between the log probability of the MAP and the log probability of the online assimilation.



\section{Conclusion}
%###################

We have demonstrated that the maximum-a-posteriori trajectory of an agent-based, spatial predator-prey model can be found given a set of partial, noisy observations. We have also shown how to construct a feasible interpretation of a stream of such observations of arbitrary length.

Since integer linear programming problems are, in full generality, NP-complete, the technique described here will not scale to very large models but will work best when agents choose from one of a limited number of options at each timestep, when interactions are sparse and when the observations impose strong constraints on the state. However, expressing the problem in this form lays the foundations for further research to apply approximations that may lead to more scalable algorithms.

Finding the MAP trajectory can be useful for a number of reasons. If we're forced to commit to a specific trajectory, given some observations, the MAP is our best bet. However, it is the most probable trajectory (perhaps among others with the same probability) among a very large number of other less probable trajectories. So usually it is very unlikely that the MAP is what really happened and we certainly shouldn't, in general, use the MAP as an interpretation of what actually happened. However, the MAP trajectory can be useful in other ways; for example, it is often a good initial condition to start an MCMC sampling algorithm, or showing the non-existence of a MAP can show that there must be either a false observation or some process has occurred that is not captured in the model.




%\bibliographystyle{unsrtnat}
%\bibliographystyle{apalike} 
\bibliographystyle{apacite}
\bibliography{references}

\end{document}
